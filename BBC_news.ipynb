{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Рассмотрим задачу классификации текстов BBC на 5 категорий: business, entertainment, politics, sport, tech.\n",
    "Используем тематическую модель ARTM и в качестве эмбеддингов документов возьмем распределение документов по темам.\n",
    "Для классификации воспользуемся градиентным бустингом.\n",
    "Цель этого ноутбука сравнить эмбеддинги тематической модели, эмбеддинги doc2vec и их конкатенацию"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "import nltk\n",
    "import artm\n",
    "from collections import Counter\n",
    "nltk.download('wordnet')\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "данные возьмем с kaggle соревнования https://www.kaggle.com/c/learn-ai-bbc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv('BBC News Train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ArticleId</th>\n",
       "      <th>Text</th>\n",
       "      <th>Category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1833</td>\n",
       "      <td>worldcom ex-boss launches defence lawyers defe...</td>\n",
       "      <td>business</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>154</td>\n",
       "      <td>german business confidence slides german busin...</td>\n",
       "      <td>business</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1101</td>\n",
       "      <td>bbc poll indicates economic gloom citizens in ...</td>\n",
       "      <td>business</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1976</td>\n",
       "      <td>lifestyle  governs mobile choice  faster  bett...</td>\n",
       "      <td>tech</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>917</td>\n",
       "      <td>enron bosses in $168m payout eighteen former e...</td>\n",
       "      <td>business</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1582</td>\n",
       "      <td>howard  truanted to play snooker  conservative...</td>\n",
       "      <td>politics</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>651</td>\n",
       "      <td>wales silent on grand slam talk rhys williams ...</td>\n",
       "      <td>sport</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1797</td>\n",
       "      <td>french honour for director parker british film...</td>\n",
       "      <td>entertainment</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>2034</td>\n",
       "      <td>car giant hit by mercedes slump a slump in pro...</td>\n",
       "      <td>business</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1866</td>\n",
       "      <td>fockers fuel festive film chart comedy meet th...</td>\n",
       "      <td>entertainment</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   ArticleId                                               Text       Category\n",
       "0       1833  worldcom ex-boss launches defence lawyers defe...       business\n",
       "1        154  german business confidence slides german busin...       business\n",
       "2       1101  bbc poll indicates economic gloom citizens in ...       business\n",
       "3       1976  lifestyle  governs mobile choice  faster  bett...           tech\n",
       "4        917  enron bosses in $168m payout eighteen former e...       business\n",
       "5       1582  howard  truanted to play snooker  conservative...       politics\n",
       "6        651  wales silent on grand slam talk rhys williams ...          sport\n",
       "7       1797  french honour for director parker british film...  entertainment\n",
       "8       2034  car giant hit by mercedes slump a slump in pro...       business\n",
       "9       1866  fockers fuel festive film chart comedy meet th...  entertainment"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "посмотрим на баланс классов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "sport            346\n",
       "business         336\n",
       "politics         274\n",
       "entertainment    273\n",
       "tech             261\n",
       "Name: Category, dtype: int64"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df['Category'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "проведем предобработку текста: лемматизацию, удалим стоп-слова и разобьем на n-граммы"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "def filter_and_tokenize(text):\n",
    "    text = text.lower().replace('\\n', ' ')\n",
    "    text = ' '.join([t for t in text.split(' ')\n",
    "                        if not t.startswith('@') and not t.startswith('http') and not t.startswith('www')])\n",
    "\n",
    "    for p in string.punctuation:\n",
    "        text = text.replace(p, ' ')\n",
    "\n",
    "    return [t for t in text.split(' ') if not(t.isnumeric()) and len(t) > 1]\n",
    "train_df['Text'] = train_df['Text'].apply(filter_and_tokenize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer \n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "def lemm(tokens):\n",
    "    return [lemmatizer.lemmatize(t) for t in tokens]\n",
    "\n",
    "train_df['Text'] = train_df['Text'].apply(lemm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords = set(nltk.corpus.stopwords.words('english') + ['wa', 'ha'])\n",
    "\n",
    "def remove_stopwords(tokens):\n",
    "    return [t for t in tokens if t not in stopwords]\n",
    "\n",
    "train_df['Text'] = train_df['Text'].apply(remove_stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def n_grams(arr):\n",
    "    bag = []\n",
    "    for i in range(len(arr)):\n",
    "        bag.append(arr[i])\n",
    "        if i < len(arr) - 1:\n",
    "            bag.append(arr[i]+\"_\"+arr[i+1])\n",
    "        if i < len(arr) - 2:\n",
    "            bag.append(arr[i]+\"_\"+arr[i+1]+\"_\"+arr[i+2])\n",
    "    return bag\n",
    "train_df['Text'] = train_df['Text'].apply(n_grams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('said', 4839),\n",
       " ('year', 2172),\n",
       " ('mr', 2007),\n",
       " ('would', 1714),\n",
       " ('also', 1426),\n",
       " ('new', 1338),\n",
       " ('people', 1325),\n",
       " ('one', 1277),\n",
       " ('u', 1264),\n",
       " ('time', 1067),\n",
       " ('could', 1032),\n",
       " ('game', 963),\n",
       " ('first', 935),\n",
       " ('last', 893),\n",
       " ('two', 889),\n",
       " ('say', 845),\n",
       " ('film', 832),\n",
       " ('world', 823),\n",
       " ('uk', 780),\n",
       " ('government', 777),\n",
       " ('make', 711),\n",
       " ('company', 683),\n",
       " ('firm', 675),\n",
       " ('best', 644),\n",
       " ('get', 626),\n",
       " ('service', 620),\n",
       " ('number', 619),\n",
       " ('told', 591),\n",
       " ('month', 590),\n",
       " ('three', 584)]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token_to_counter = Counter()\n",
    "for row in train_df['Text']:\n",
    "    token_to_counter.update(row)\n",
    "token_to_counter.most_common(30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "vw_filaname = 'texts.vw.txt'\n",
    "Counter = 0\n",
    "with open(vw_filaname, 'w') as fout:\n",
    "    for row in train_df.iterrows():\n",
    "        tokens = row[1][1]\n",
    "        ID = row[1][0]\n",
    "        fout.write('{} {} \\n'.format(ID, \" \".join(tokens)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "bv = artm.BatchVectorizer(data_path=vw_filaname, data_format='vowpal_wabbit', batch_size=10000, target_folder='batches')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "построим тематическую модель"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = artm.ARTM(num_topics=30, cache_theta = True, num_document_passes=10, dictionary=bv.dictionary, class_ids={'@default_class': 1.0})\n",
    "model.scores.add(artm.PerplexityScore(name='perplexity', dictionary=bv.dictionary, class_ids=['@default_class']))\n",
    "model.scores.add(artm.TopTokensScore(name='top-tokens', num_tokens=15))\n",
    "model.scores.add(artm.SparsityPhiScore(name='sparsity', class_id='@default_class'))\n",
    "model.regularizers.add(artm.DecorrelatorPhiRegularizer(name='decorrelator_phi_def',class_ids=['@default_class'], tau=2e+5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter #0, perplexity: 533075.3125, sparsity: 0.0\n",
      "Iter #1, perplexity: 105412.03125, sparsity: 0.0004982234095223248\n",
      "Iter #2, perplexity: 38084.25, sparsity: 0.6386135220527649\n",
      "Iter #3, perplexity: 18662.138671875, sparsity: 0.8575142025947571\n",
      "Iter #4, perplexity: 15485.009765625, sparsity: 0.9145038723945618\n",
      "Iter #5, perplexity: 14497.2646484375, sparsity: 0.9365740418434143\n",
      "Iter #6, perplexity: 14244.81640625, sparsity: 0.9458815455436707\n",
      "Iter #7, perplexity: 14093.7548828125, sparsity: 0.950194776058197\n",
      "Iter #8, perplexity: 14045.1728515625, sparsity: 0.9526783227920532\n",
      "Iter #9, perplexity: 14008.3427734375, sparsity: 0.9544075131416321\n",
      "Iter #10, perplexity: 14004.0126953125, sparsity: 0.9554308652877808\n",
      "Iter #11, perplexity: 13994.330078125, sparsity: 0.9561180472373962\n",
      "Iter #12, perplexity: 13992.599609375, sparsity: 0.956582248210907\n",
      "Iter #13, perplexity: 13986.0478515625, sparsity: 0.9569264054298401\n",
      "Iter #14, perplexity: 13984.3154296875, sparsity: 0.9571753740310669\n",
      "Iter #15, perplexity: 13980.43359375, sparsity: 0.9573525190353394\n"
     ]
    }
   ],
   "source": [
    "for i in range(16):\n",
    "    model.fit_offline(bv, num_collection_passes=1)\n",
    "    print(f'Iter #{i}, perplexity: {model.score_tracker[\"perplexity\"].last_value}, sparsity: {model.score_tracker[\"sparsity\"].last_value}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/roman/anaconda3/lib/python3.7/site-packages/scipy/sparse/sparsetools.py:21: DeprecationWarning: `scipy.sparse.sparsetools` is deprecated!\n",
      "scipy.sparse.sparsetools is a private module for scipy.sparse, and should not be used.\n",
      "  _deprecated()\n"
     ]
    }
   ],
   "source": [
    "from gensim.test.utils import common_texts\n",
    "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
    "X = train_df\n",
    "y = train_df['Category']\n",
    "documents = [TaggedDocument(doc, [i]) for i, doc in enumerate(X['Text'])]\n",
    "d2vModel = Doc2Vec(documents, vector_size=60, window=20, min_count=10, workers=8, random_state = 42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "получим точность классификации с помощью кросс-валидации и сравним результаты"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "doc2vec: 0.9442953020134228\n",
      "topic: 0.9322147651006711\n",
      "concat: 0.963758389261745\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn import metrics\n",
    "import lightgbm as lgb\n",
    "k = 5\n",
    "skf = StratifiedKFold(n_splits=k)\n",
    "\n",
    "X_topic = model.get_theta().to_numpy().T\n",
    "avg_concat = 0\n",
    "avg_topic = 0\n",
    "avg_doc2vec = 0\n",
    "for train, test in skf.split(X, y):\n",
    "    y_tr = y[train].to_numpy().T\n",
    "    y_tst = y[test].to_numpy().T\n",
    "    X_tr_t = X_topic[train]\n",
    "    X_tst_t = X_topic[test]\n",
    "    X_tr_doc = np.array([d2vModel.infer_vector(u) for u in X.iloc[train]['Text']])\n",
    "    X_tst_doc = np.array([d2vModel.infer_vector(u) for u in X.iloc[test]['Text']])\n",
    "    X_tr_con = np.concatenate((X_tr_doc,X_tr_t), axis=1)\n",
    "    X_tst_con = np.concatenate((X_tst_doc,X_tst_t), axis=1)\n",
    "    lgbm1 = lgb.LGBMClassifier(n_estimators=100, reg_alpha=0.01, learning_rate=0.2, class_weight='balanced', num_leaves = 20, max_depth = 6)\n",
    "    lgbm1.fit(X_tr_doc, y_tr)\n",
    "    y_pred = lgbm1.predict(X_tst_doc)\n",
    "    acc =  metrics.accuracy_score(y_pred, y_tst)\n",
    "    avg_doc2vec += acc\n",
    "    \n",
    "    lgbm2 = lgb.LGBMClassifier(n_estimators=100, reg_alpha=0.01, learning_rate=0.2, class_weight='balanced', num_leaves = 20, max_depth = 6)\n",
    "    lgbm2.fit(X_tr_t, y_tr)\n",
    "    y_pred = lgbm2.predict(X_tst_t)\n",
    "    acc =  metrics.accuracy_score(y_pred, y_tst)\n",
    "    avg_topic += acc\n",
    "    \n",
    "    lgbm3 = lgb.LGBMClassifier(n_estimators=100, reg_alpha=0.01, learning_rate=0.2, class_weight='balanced', num_leaves = 20, max_depth = 6)\n",
    "    lgbm3.fit(X_tr_con, y_tr)\n",
    "    y_pred = lgbm3.predict(X_tst_con)\n",
    "    acc =  metrics.accuracy_score(y_pred, y_tst)\n",
    "    avg_concat += acc\n",
    "print(\"doc2vec:\", avg_doc2vec / k)\n",
    "print(\"topic:\", avg_topic / k)\n",
    "print(\"concat:\", avg_concat / k)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "как можно заметить, конкатенация эмбедингов тематической модели и doc2vec дала наилучший результат."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
